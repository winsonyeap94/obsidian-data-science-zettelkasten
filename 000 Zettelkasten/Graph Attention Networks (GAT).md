---
tags:
  - graphs
  - neural-networks
  - attention
creation-date: 2024-06-05 22:46
---
# Graph Attention Networks (GAT)

![[Pasted image 20240605224946.png]]

Graph Attention Networks leverages **masked self-attention layers** to address the shortcomings of prior methods based on graph convolutions or their approximations.


The benefit of attention mechanisms is that they allow for dealing with *variable-sized inputs*, focusing on the most relevant parts of the input to make decisions.
- When an attention mechanism is used to compute a representation of a single sequence, it is commonly referred to as *self-attention* or *intra-attention*.
- In GATs, the idea is to compute the hidden representations of each node in the graph, by attending over its neighbours, following a self-attention strategy.
- The attention architecture has several interesting properties:
	1. The operation is *efficient*, since it is parallelisable across node-neighbour pairs.
	2. It can be applied to graph nodes having *different degrees*  by specifying arbitrary weights to the neighbours.
	3. The model is directly applicable to *inductive* learning problems, including tasks where the model has to generalise to completely unseen graphs.

## Algorithm

Initialisation:
$$
h_v^{(0)} = x_v \ \ \ \ \ \text{for all } v \in V
$$
- $h_v^{(0)}$ : Node $v$'s initial embedding
- $x_v$ : Node $v$'s original features

And for $k=1,2,..., K$, where $K$ is the total steps,
$$
h_v^{(k)} = f^{(k)} \Bigg( W^{(k)} \cdot \bigg[ \underset{u\in\mathcal{N}(v)}{\sum} \alpha_{vu}^{(k-1)}\cdot h_u^{(k-1)} + \alpha_{vv}^{(k-1)}\cdot h_v^{(k-1)} \bigg] \Bigg) \ \ \ \ \ \text{for all }v \in V
$$
- $h_v^{(k)}$ : Node $v$'s embedding at step $k$
- $h_u^{(k-1)}$ : Weighted mean of $v$'s neighbour's embeddings at step $k-1$
- $h_v^{(k-1)}$ : Node $v$'s embedding at step $k-1$

The attention weights $\alpha^{(k)}$ are generated by an attention mechanism $A^{(k)}$, normalised such that the sum over all neighbours of each node $v$ is $1$.
$$
\alpha_{vu}^{(k)} = \frac{A^{(k)}(h_v^{(k)}, h_u^{(k)})}{\underset{w\in\mathcal{N}(v)}{\sum} A^{(k)}(h_v^{(k)}, h_w^{(k)})} \ \ \ \ \ \text{for all } (v,u) \in E
$$


Predictions can be made at each node using the final computed embedding:
$$
\hat{y_v} = \text{PREDICT}(h_v^{(K)})
$$
where $\text{PREDICT}$ is generally another neural network, learnt together with the GAT model.

## Multi-head Attention

Multi-head attention aims to help **stabilise the learning process** of the attention mechanism.

We create *multiple attention scores* (multiple attention mechanisms), and then aggregate the outputs by concatenation or summation.
- Multi-heads:
	- $h_v^{(k)}[1] = \sigma \big(\underset{u\in\mathcal{N}(v)}{\sum} \alpha_{vu}^1 W^{(k)} h_u^{(k-1)} \big)$
	- $h_v^{(k)}[2] = \sigma \big(\underset{u\in\mathcal{N}(v)}{\sum} \alpha_{vu}^2 W^{(k)} h_u^{(k-1)} \big)$
	- $h_v^{(k)}[3] = \sigma \big(\underset{u\in\mathcal{N}(v)}{\sum} \alpha_{vu}^3 W^{(k)} h_u^{(k-1)} \big)$
- Aggregation:
	- $h_v^{(k)} = \text{AGG}\big( h_v^{(k)}[1], h_v^{(k)}[2], h_v^{(k)}[3] \big)$

---
# References

- [[CS224W - Machine Learning with Graphs#Chapter 4 A general perspective on GNNs]]
- [Understanding Convolutions on Graphs (distill.pub)](https://distill.pub/2021/understanding-gnns/)
- Original paper: [1710.10903 (arxiv.org)](https://arxiv.org/pdf/1710.10903)